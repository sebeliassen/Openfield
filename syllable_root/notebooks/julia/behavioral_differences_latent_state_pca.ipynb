{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "FPS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the .h5 file\n",
    "file_path = '../syllable_analysis_julia_11_vids/results.h5'\n",
    "\n",
    "# Initialize a dictionary to hold dfs for each Mouse ID\n",
    "include_latent_state = True\n",
    "\n",
    "# Using defaultdict in case there are multiple groups per Mouse ID\n",
    "dfs = defaultdict(list)\n",
    "\n",
    "# Regular expression pattern to extract Mouse ID\n",
    "# Assumes Mouse ID is the number before 'DLC' in the group name\n",
    "mouse_id_pattern = re.compile(r'_(\\d+)DLC_')\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    for group_name in file.keys():\n",
    "        # Extract Mouse ID using regex\n",
    "        match = mouse_id_pattern.search(group_name)\n",
    "        if match:\n",
    "            mouse_id = match.group(1)\n",
    "        else:\n",
    "            # Handle cases where Mouse ID is not found\n",
    "            print(f\"Warning: Mouse ID not found in group name '{group_name}'. Skipping this group.\")\n",
    "            continue  # Skip this group\n",
    "        \n",
    "        # Check if Mouse ID already exists in the dictionary (this would indicate an error)\n",
    "        if mouse_id in dfs:\n",
    "            raise ValueError(f\"Error: Multiple groups found for Mouse ID '{mouse_id}' in the file. Only one group per Mouse ID is expected.\")\n",
    "        \n",
    "        group = file[group_name]\n",
    "        \n",
    "        # List to hold individual DataFrames for each dataset\n",
    "        df_list = []\n",
    "        \n",
    "        for dataset_name in group.keys():\n",
    "            # Exclude 'latent_state' datasets if the flag is False\n",
    "            if not include_latent_state and dataset_name.startswith('latent_state'):\n",
    "                continue\n",
    "            \n",
    "            dataset = group[dataset_name][:]\n",
    "            \n",
    "            # Check if dataset is at least 1D\n",
    "            if dataset.ndim == 1:\n",
    "                # Convert to 2D array with one column\n",
    "                dataset = dataset.reshape(-1, 1)\n",
    "            \n",
    "            # Create column names by appending index to dataset name\n",
    "            # Example: 'centroid_0', 'centroid_1', ...\n",
    "            num_cols = dataset.shape[1]\n",
    "            columns = [f\"{dataset_name}_{i}\" for i in range(num_cols)]\n",
    "            \n",
    "            # Convert dataset to DataFrame\n",
    "            df = pd.DataFrame(dataset, columns=columns)\n",
    "            df_list.append(df)\n",
    "        \n",
    "        if df_list:\n",
    "            # Concatenate all DataFrames horizontally (axis=1)\n",
    "            concatenated_df = pd.concat(df_list, axis=1)\n",
    "            # Add to the dictionary with the Mouse ID as the key\n",
    "            dfs[mouse_id] = concatenated_df\n",
    "        else:\n",
    "            print(f\"Warning: No datasets found in group '{group_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    df['timestamp'] = df.index / FPS\n",
    "    df['current_minute'] = df['timestamp'] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_info = pd.read_csv('../syllable_analysis_julia_11_vids/syllable,behavior_group_dim4.csv', index_col=0)\n",
    "#pd.read_csv('../syllable_analysis_julia/syllable,behavior.csv', index_col=0)\n",
    "#syllable_info.loc[41] = 'faulty'\n",
    "\n",
    "syllable_map = syllable_info.to_dict()['behavior']\n",
    "\n",
    "mouse_info = pd.read_csv('../syllable_analysis_julia_11_vids/syllables_mouseCoh1_info.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    df['syllable_name'] = df['syllable_0'].map(syllable_map)\n",
    "    df['syllable_name'] = df['syllable_name'].fillna('misc')  # Replace NaN with 'faulty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def bin_df_by_injection_sliding_window_signed(df, mouse_id, window_size, stride):\n",
    "    \"\"\"\n",
    "    Bins the dataframe using a sliding window approach separately for pre-injection and post-injection periods,\n",
    "    assigning negative bin numbers to pre-injection windows and positive to post-injection windows.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe containing the data.\n",
    "    - mouse_id (int or str): The ID of the mouse.\n",
    "    - window_size (int): Size of the sliding window in minutes.\n",
    "    - stride (int): Stride length for the sliding window in minutes.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with bin labels as keys and binned dataframes as values.\n",
    "    \"\"\"\n",
    "    # Retrieve injection times from mouse_info DataFrame\n",
    "    injection_times = mouse_info.loc[mouse_id][['PreInjStart', 'PreInjEnd', 'PostInjStart', 'PostInjEnd']].to_list()\n",
    "    pre_inj_start, pre_inj_end, post_inj_start, post_inj_end = injection_times\n",
    "\n",
    "    # Initialize the dictionary to store binned dataframes\n",
    "    binned_dfs = {}\n",
    "\n",
    "    # Initialize bin counters\n",
    "    bin_counter_pre = -1  # Start from -1 for pre-injection bins\n",
    "    bin_counter_post = 1  # Start from 1 for post-injection bins\n",
    "\n",
    "    ### Binning Pre-Injection Data ###\n",
    "    current_start = pre_inj_start\n",
    "    current_end = current_start + window_size\n",
    "\n",
    "    while current_end <= pre_inj_end:\n",
    "        # Define the window\n",
    "        window_df = df[(df['current_minute'] >= current_start) & (df['current_minute'] < current_end)]\n",
    "        bin_label = f'bin_{bin_counter_pre}'\n",
    "        binned_dfs[bin_label] = window_df\n",
    "\n",
    "        # Update counters and window positions\n",
    "        bin_counter_pre -= 1\n",
    "        current_start += stride\n",
    "        current_end = current_start + window_size\n",
    "\n",
    "    ### Binning Post-Injection Data ###\n",
    "    current_start = post_inj_start\n",
    "    current_end = current_start + window_size\n",
    "\n",
    "    while current_end <= post_inj_end:\n",
    "        # Define the window\n",
    "        window_df = df[(df['current_minute'] >= current_start) & (df['current_minute'] < current_end)]\n",
    "        bin_label = f'bin_{bin_counter_post}'\n",
    "        binned_dfs[bin_label] = window_df\n",
    "\n",
    "        # Update counters and window positions\n",
    "        bin_counter_post += 1\n",
    "        current_start += stride\n",
    "        current_end = current_start + window_size\n",
    "\n",
    "    return binned_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a defaultdict to store binned dataframes\n",
    "all_centroids = {}\n",
    "window_size = 5\n",
    "stride = 2\n",
    "\n",
    "for mouse_id, df in dfs.items():\n",
    "    # Apply the binning function\n",
    "    binned_dfs = bin_df_by_injection_sliding_window_signed(df, int(mouse_id), window_size, stride)\n",
    "    \n",
    "    # Get genotype information\n",
    "    genotype = mouse_info.loc[int(mouse_id)]['Genotype']  # Adjust column name as needed\n",
    "    \n",
    "    for bin_label, bin_df in binned_dfs.items():\n",
    "        bin_number = int(bin_label.split('_')[1])\n",
    "        \n",
    "        # Select columns that start with 'latent'\n",
    "        latent_cols = [col for col in bin_df.columns if col.startswith('latent')]\n",
    "        \n",
    "        # Check if there are latent columns\n",
    "        if latent_cols:\n",
    "            # Optionally sort the columns to maintain consistent order\n",
    "            latent_cols.sort()\n",
    "            # Compute the mean of latent columns\n",
    "            latent_mean = bin_df[latent_cols].mean().values\n",
    "            #all_centroids[(bin_number, genotype)].append(latent_mean)\n",
    "            all_centroids[(bin_number, genotype)] = latent_mean\n",
    "        else:\n",
    "            # Handle bins without latent columns if necessary\n",
    "            print(f\"No latent columns in bin {bin_label} for mouse {mouse_id}\")\n",
    "            continue  # Skip this bin\n",
    "\n",
    "all_centroids = {k: np.vstack(v) for k, v in all_centroids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. PCA expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply PCA to reduce dimensions from 4D to 2D\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m centroids_2d \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentroids_4d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Populate the new 2D centroids dictionary\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys):\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:511\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/sund/lib/python3.11/site-packages/sklearn/utils/validation.py:1053\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1059\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1060\u001b[0m         array,\n\u001b[1;32m   1061\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1062\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1063\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. PCA expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "\n",
    "# Initialize a defaultdict to store 2D binned dataframes\n",
    "all_centroids_2d = {}\n",
    "\n",
    "# Extract all centroid vectors and corresponding keys\n",
    "keys = list(all_centroids.keys())\n",
    "centroids_4d = np.array([all_centroids[key] for key in keys])\n",
    "\n",
    "# Apply PCA to reduce dimensions from 4D to 2D\n",
    "pca = PCA(n_components=2)\n",
    "centroids_2d = pca.fit_transform(centroids_4d)\n",
    "\n",
    "# Populate the new 2D centroids dictionary\n",
    "for i, key in enumerate(keys):\n",
    "    all_centroids_2d[key] = centroids_2d[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define marker styles for genotypes\n",
    "genotype_markers = {'WT': 'o', 'Het': 's'}  # Customize as needed\n",
    "\n",
    "# Organize data by genotype\n",
    "data_by_genotype = defaultdict(list)\n",
    "bin_numbers_by_genotype = defaultdict(list)\n",
    "\n",
    "for (bin_number, genotype), coords in all_centroids_2d.items():\n",
    "    data_by_genotype[genotype].append(coords)\n",
    "    bin_numbers_by_genotype[genotype].append(bin_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import Normalize, TwoSlopeNorm\n",
    "\n",
    "# Step 4: Define the Plotting Function with TwoSlopeNorm\n",
    "def plot_smoothed_trajectories_from_centroids(\n",
    "    data_by_genotype,\n",
    "    bin_numbers_by_genotype,\n",
    "    genotype_markers,\n",
    "    smoothing=True,\n",
    "    window_size=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots smoothed trajectories for each genotype using a rolling mean.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_by_genotype (dict): Dictionary mapping genotype to list of 2D centroid coordinates.\n",
    "    - bin_numbers_by_genotype (dict): Dictionary mapping genotype to list of bin numbers.\n",
    "    - genotype_markers (dict): Dictionary mapping genotypes to marker styles.\n",
    "    - smoothing (bool): Whether to apply rolling mean smoothing.\n",
    "    - window_size (int): The window size for the rolling mean.\n",
    "    \"\"\"\n",
    "    # Step 4.1: Determine the Maximum Absolute Bin Number\n",
    "    max_abs_bin = 0\n",
    "    for genotype, bin_numbers in bin_numbers_by_genotype.items():\n",
    "        if bin_numbers:\n",
    "            min_bin = min(bin_numbers)\n",
    "            max_bin = max(bin_numbers)\n",
    "            max_abs_bin = max(max_abs_bin, abs(min_bin), abs(max_bin))\n",
    "    \n",
    "    # Ensure that max_abs_bin is at least 1 to avoid division by zero\n",
    "    max_abs_bin = max(max_abs_bin, 1)\n",
    "    \n",
    "    # Step 4.2: Set Up the Colormap and Normalization\n",
    "    cmap = plt.get_cmap('seismic')  # Using a diverging colormap\n",
    "    norm = TwoSlopeNorm(vmin=-max_abs_bin, vcenter=-window_size/2, vmax=max_abs_bin)\n",
    "    sm_color = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm_color.set_array([])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for genotype in data_by_genotype:\n",
    "        coords_list = data_by_genotype[genotype]\n",
    "        bin_numbers = bin_numbers_by_genotype[genotype]\n",
    "        \n",
    "        # Sort the data by bin_number to maintain chronological order\n",
    "        sorted_indices = np.argsort(bin_numbers)\n",
    "        sorted_bin_numbers = np.array(bin_numbers)[sorted_indices]\n",
    "        sorted_coords = np.array(coords_list)[sorted_indices]\n",
    "        \n",
    "        x = sorted_coords[:, 0]\n",
    "        y = sorted_coords[:, 1]\n",
    "        \n",
    "        # Apply rolling mean smoothing if required\n",
    "        if smoothing and len(x) >= window_size:\n",
    "            df_coords = pd.DataFrame({'x': x, 'y': y})\n",
    "            df_coords['x_smooth'] = df_coords['x'].rolling(\n",
    "                window=window_size, center=True, min_periods=1\n",
    "            ).mean()\n",
    "            df_coords['y_smooth'] = df_coords['y'].rolling(\n",
    "                window=window_size, center=True, min_periods=1\n",
    "            ).mean()\n",
    "            x_smooth = df_coords['x_smooth'].values\n",
    "            y_smooth = df_coords['y_smooth'].values\n",
    "        else:\n",
    "            x_smooth, y_smooth = x, y\n",
    "        \n",
    "        # Plot trajectory lines with color gradient based on bin numbers\n",
    "        for i in range(len(x_smooth) - 1):\n",
    "            ax.plot(\n",
    "                x_smooth[i:i+2],\n",
    "                y_smooth[i:i+2],\n",
    "                color=cmap(norm(sorted_bin_numbers[i])),\n",
    "                linewidth=2\n",
    "            )\n",
    "        \n",
    "        # Plot markers with colors based on bin numbers\n",
    "        scatter = ax.scatter(\n",
    "            x_smooth,\n",
    "            y_smooth,\n",
    "            c=sorted_bin_numbers,\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "            marker=genotype_markers.get(genotype, 'o'),\n",
    "            s=100,\n",
    "            edgecolors='k',\n",
    "            linewidths=0.5,\n",
    "            alpha=0.9,\n",
    "            label=genotype\n",
    "        )\n",
    "    \n",
    "    # Step 4.3: Create Custom Legend for Genotypes\n",
    "    legend_elements = [\n",
    "        Line2D(\n",
    "            [0], [0],\n",
    "            marker=marker,\n",
    "            color='w',\n",
    "            label=genotype,\n",
    "            markerfacecolor='black',\n",
    "            markersize=12\n",
    "        )\n",
    "        for genotype, marker in genotype_markers.items()\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, title='Genotypes', loc='best')\n",
    "    \n",
    "    # Step 4.4: Add Colorbar\n",
    "    cbar = plt.colorbar(sm_color, ax=ax)\n",
    "    cbar.set_label('Bin Number', fontsize=12)\n",
    "    \n",
    "    # Step 4.5: Label Axes and Set Title\n",
    "    plt.xlabel('Principal Component 1', fontsize=14)\n",
    "    plt.ylabel('Principal Component 2', fontsize=14)\n",
    "    plt.title(\n",
    "        'Smoothed Trajectories from Centroids with Rolling Mean\\n'\n",
    "        'Colored by Bin Number and Styled by Genotype',\n",
    "        fontsize=16\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plotting function\n",
    "plot_smoothed_trajectories_from_centroids(\n",
    "    data_by_genotype=data_by_genotype,\n",
    "    bin_numbers_by_genotype=bin_numbers_by_genotype,\n",
    "    genotype_markers=genotype_markers,\n",
    "    smoothing=True,\n",
    "    window_size=15  # Adjust window_size as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUND",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
